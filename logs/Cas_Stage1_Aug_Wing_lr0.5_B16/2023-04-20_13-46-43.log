[Thu Apr 20 13:46:43 2023|main.py|INFO] Task: Cas_Stage1_Aug_Wing_lr0.5_B16
[Thu Apr 20 13:46:43 2023|main.py|INFO] Training cascade stage 1
[Thu Apr 20 13:46:43 2023|main.py|INFO] Using device: cuda
[Thu Apr 20 13:46:43 2023|main.py|INFO] Using config: config\Cas_Stage1_Aug_Wing\Cas_Stage1_Aug_Wing_lr0.5_B16.yaml
[Thu Apr 20 13:46:43 2023|main.py|INFO] Train path: data/training_images_full_train.npz
[Thu Apr 20 13:46:43 2023|main.py|INFO] Train path 2: data/training_images_subset.npz
[Thu Apr 20 13:46:43 2023|main.py|INFO] Test path: data/training_images_full_test.npz
[Thu Apr 20 13:46:43 2023|main.py|INFO] Train augment: True
[Thu Apr 20 13:46:43 2023|main.py|INFO] Learning rate: 0.5
[Thu Apr 20 13:46:43 2023|main.py|INFO] Batch size: 16
[Thu Apr 20 13:46:43 2023|main.py|INFO] Num epochs: 100
[Thu Apr 20 13:46:43 2023|main.py|INFO] Save model: True
[Thu Apr 20 13:46:43 2023|main.py|INFO] Loss: Wing
[Thu Apr 20 13:46:43 2023|main.py|INFO] Log path: logs/Cas_Stage1_Aug_Wing_lr0.5_B16/2023-04-20_13-46-43.log
[Thu Apr 20 13:46:43 2023|main.py|INFO] Writer path: runs/Cas_Stage1_Aug_Wing_lr0.5_B16/2023-04-20_13-46-43
[Thu Apr 20 13:46:43 2023|main.py|INFO] Model name: resnet18
[Thu Apr 20 13:46:43 2023|main.py|INFO] Num outputs: 10
[Thu Apr 20 13:46:43 2023|main.py|INFO] Pretrained: True
[Thu Apr 20 13:46:43 2023|main.py|INFO] Load model: False
[Thu Apr 20 13:46:43 2023|main.py|INFO] Load path: None
[Thu Apr 20 13:46:43 2023|main.py|INFO] Loading data...
[Thu Apr 20 13:46:43 2023|main.py|INFO] Load dataset for cascade stage 1
[Thu Apr 20 13:46:48 2023|main.py|INFO] Initializing network resnet18 with 10 outputs...
[Thu Apr 20 13:46:49 2023|main.py|INFO] Network: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[Thu Apr 20 13:46:49 2023|main.py|INFO] Initializing loss and optimizer...
[Thu Apr 20 13:46:49 2023|main.py|INFO] Loss: Wing
[Thu Apr 20 13:46:49 2023|main.py|INFO] Optimizer: Adam
[Thu Apr 20 13:46:49 2023|main.py|INFO] Initializing tensorboard writer at: runs/Cas_Stage1_Aug_Wing_lr0.5_B16/2023-04-20_13-46-43
[Thu Apr 20 13:46:49 2023|main.py|INFO] Training network...
[Thu Apr 20 14:08:59 2023|main.py|INFO] EPOCH [1/100] Train NME: 0.31695
[Thu Apr 20 14:09:00 2023|main.py|INFO] EPOCH [1/100] Test NME: 0.36853
[Thu Apr 20 14:09:00 2023|main.py|INFO] EPOCH [1/100] Learning rate: 0.50000
[Thu Apr 20 14:09:00 2023|main.py|INFO] EPOCH [1/100] NME improved from 1000.00000 to 0.36853
[Thu Apr 20 14:09:00 2023|main.py|INFO] EPOCH [1/100] Saving model to: checkpoints/Cas_Stage1_Aug_Wing_lr0.5_B16/2023-04-20_14-09-00_epoch_1_NME_0.36853.pth.tar
[Thu Apr 20 14:35:31 2023|main.py|INFO] EPOCH [2/100] Train NME: 0.22675
[Thu Apr 20 14:35:32 2023|main.py|INFO] EPOCH [2/100] Test NME: 0.16499
[Thu Apr 20 14:35:32 2023|main.py|INFO] EPOCH [2/100] Learning rate: 0.40000
[Thu Apr 20 14:35:32 2023|main.py|INFO] EPOCH [2/100] NME improved from 0.36853 to 0.16499
[Thu Apr 20 14:35:32 2023|main.py|INFO] EPOCH [2/100] Saving model to: checkpoints/Cas_Stage1_Aug_Wing_lr0.5_B16/2023-04-20_14-35-32_epoch_2_NME_0.16499.pth.tar
[Thu Apr 20 14:51:01 2023|main.py|INFO] EPOCH [3/100] Train NME: 0.18633
[Thu Apr 20 14:51:01 2023|main.py|INFO] EPOCH [3/100] Test NME: 0.18023
[Thu Apr 20 14:51:01 2023|main.py|INFO] EPOCH [3/100] Learning rate: 0.32000
[Thu Apr 20 15:03:37 2023|main.py|INFO] EPOCH [4/100] Train NME: 0.17123
[Thu Apr 20 15:03:37 2023|main.py|INFO] EPOCH [4/100] Test NME: 0.14265
[Thu Apr 20 15:03:37 2023|main.py|INFO] EPOCH [4/100] Learning rate: 0.25600
[Thu Apr 20 15:03:37 2023|main.py|INFO] EPOCH [4/100] NME improved from 0.16499 to 0.14265
[Thu Apr 20 15:03:37 2023|main.py|INFO] EPOCH [4/100] Saving model to: checkpoints/Cas_Stage1_Aug_Wing_lr0.5_B16/2023-04-20_15-03-37_epoch_4_NME_0.14265.pth.tar
[Thu Apr 20 15:16:12 2023|main.py|INFO] EPOCH [5/100] Train NME: 0.16233
[Thu Apr 20 15:16:13 2023|main.py|INFO] EPOCH [5/100] Test NME: 0.15395
[Thu Apr 20 15:16:13 2023|main.py|INFO] EPOCH [5/100] Learning rate: 0.20480
[Thu Apr 20 15:29:02 2023|main.py|INFO] EPOCH [6/100] Train NME: 0.15483
[Thu Apr 20 15:29:02 2023|main.py|INFO] EPOCH [6/100] Test NME: 0.14538
[Thu Apr 20 15:29:02 2023|main.py|INFO] EPOCH [6/100] Learning rate: 0.16384
[Thu Apr 20 15:41:30 2023|main.py|INFO] EPOCH [7/100] Train NME: 0.15433
[Thu Apr 20 15:41:31 2023|main.py|INFO] EPOCH [7/100] Test NME: 0.15675
[Thu Apr 20 15:41:31 2023|main.py|INFO] EPOCH [7/100] Learning rate: 0.13107
[Thu Apr 20 15:54:19 2023|main.py|INFO] EPOCH [8/100] Train NME: 0.15172
[Thu Apr 20 15:54:20 2023|main.py|INFO] EPOCH [8/100] Test NME: 0.15358
[Thu Apr 20 15:54:20 2023|main.py|INFO] EPOCH [8/100] Learning rate: 0.10486
[Thu Apr 20 16:06:16 2023|main.py|INFO] EPOCH [9/100] Train NME: 0.14574
[Thu Apr 20 16:06:16 2023|main.py|INFO] EPOCH [9/100] Test NME: 0.14590
[Thu Apr 20 16:06:16 2023|main.py|INFO] EPOCH [9/100] Learning rate: 0.08389
[Thu Apr 20 16:19:06 2023|main.py|INFO] EPOCH [10/100] Train NME: 0.14394
[Thu Apr 20 16:19:06 2023|main.py|INFO] EPOCH [10/100] Test NME: 0.14977
[Thu Apr 20 16:19:06 2023|main.py|INFO] EPOCH [10/100] Learning rate: 0.06711
[Thu Apr 20 16:31:20 2023|main.py|INFO] EPOCH [11/100] Train NME: 0.13782
[Thu Apr 20 16:31:21 2023|main.py|INFO] EPOCH [11/100] Test NME: 0.14688
[Thu Apr 20 16:31:21 2023|main.py|INFO] EPOCH [11/100] Learning rate: 0.05369
[Thu Apr 20 16:43:34 2023|main.py|INFO] EPOCH [12/100] Train NME: 0.13831
[Thu Apr 20 16:43:34 2023|main.py|INFO] EPOCH [12/100] Test NME: 0.14951
[Thu Apr 20 16:43:34 2023|main.py|INFO] EPOCH [12/100] Learning rate: 0.04295
[Thu Apr 20 16:55:39 2023|main.py|INFO] EPOCH [13/100] Train NME: 0.13671
[Thu Apr 20 16:55:39 2023|main.py|INFO] EPOCH [13/100] Test NME: 0.15400
[Thu Apr 20 16:55:39 2023|main.py|INFO] EPOCH [13/100] Learning rate: 0.03436
[Thu Apr 20 17:08:04 2023|main.py|INFO] EPOCH [14/100] Train NME: 0.13526
[Thu Apr 20 17:08:04 2023|main.py|INFO] EPOCH [14/100] Test NME: 0.15503
[Thu Apr 20 17:08:04 2023|main.py|INFO] EPOCH [14/100] Learning rate: 0.02749
[Thu Apr 20 17:29:07 2023|main.py|INFO] EPOCH [15/100] Train NME: 0.13458
[Thu Apr 20 17:29:11 2023|main.py|INFO] EPOCH [15/100] Test NME: 0.14409
[Thu Apr 20 17:29:11 2023|main.py|INFO] EPOCH [15/100] Learning rate: 0.02199
[Thu Apr 20 18:01:24 2023|main.py|INFO] EPOCH [16/100] Train NME: 0.13385
[Thu Apr 20 18:01:27 2023|main.py|INFO] EPOCH [16/100] Test NME: 0.15057
[Thu Apr 20 18:01:27 2023|main.py|INFO] EPOCH [16/100] Learning rate: 0.01759
[Thu Apr 20 18:33:45 2023|main.py|INFO] EPOCH [17/100] Train NME: 0.13353
[Thu Apr 20 18:33:46 2023|main.py|INFO] EPOCH [17/100] Test NME: 0.15489
[Thu Apr 20 18:33:46 2023|main.py|INFO] EPOCH [17/100] Learning rate: 0.01407
[Thu Apr 20 18:52:06 2023|main.py|INFO] EPOCH [18/100] Train NME: 0.13220
[Thu Apr 20 18:52:07 2023|main.py|INFO] EPOCH [18/100] Test NME: 0.15273
[Thu Apr 20 18:52:07 2023|main.py|INFO] EPOCH [18/100] Learning rate: 0.01126
[Thu Apr 20 19:09:12 2023|main.py|INFO] EPOCH [19/100] Train NME: 0.13234
[Thu Apr 20 19:09:12 2023|main.py|INFO] EPOCH [19/100] Test NME: 0.15009
[Thu Apr 20 19:09:12 2023|main.py|INFO] EPOCH [19/100] Learning rate: 0.00901
[Thu Apr 20 19:25:13 2023|main.py|INFO] EPOCH [20/100] Train NME: 0.13262
[Thu Apr 20 19:25:14 2023|main.py|INFO] EPOCH [20/100] Test NME: 0.15776
[Thu Apr 20 19:25:14 2023|main.py|INFO] EPOCH [20/100] Learning rate: 0.00721
[Thu Apr 20 19:40:00 2023|main.py|INFO] EPOCH [21/100] Train NME: 0.13191
[Thu Apr 20 19:40:01 2023|main.py|INFO] EPOCH [21/100] Test NME: 0.14896
[Thu Apr 20 19:40:01 2023|main.py|INFO] EPOCH [21/100] Learning rate: 0.00576
[Thu Apr 20 19:54:33 2023|main.py|INFO] EPOCH [22/100] Train NME: 0.13144
[Thu Apr 20 19:54:33 2023|main.py|INFO] EPOCH [22/100] Test NME: 0.14784
[Thu Apr 20 19:54:33 2023|main.py|INFO] EPOCH [22/100] Learning rate: 0.00461
[Thu Apr 20 20:10:58 2023|main.py|INFO] EPOCH [23/100] Train NME: 0.13140
[Thu Apr 20 20:10:59 2023|main.py|INFO] EPOCH [23/100] Test NME: 0.14864
[Thu Apr 20 20:10:59 2023|main.py|INFO] EPOCH [23/100] Learning rate: 0.00369
[Thu Apr 20 20:27:14 2023|main.py|INFO] EPOCH [24/100] Train NME: 0.13110
[Thu Apr 20 20:27:14 2023|main.py|INFO] EPOCH [24/100] Test NME: 0.15044
[Thu Apr 20 20:27:14 2023|main.py|INFO] EPOCH [24/100] Learning rate: 0.00295
[Thu Apr 20 20:42:40 2023|main.py|INFO] EPOCH [25/100] Train NME: 0.13114
[Thu Apr 20 20:42:41 2023|main.py|INFO] EPOCH [25/100] Test NME: 0.15180
[Thu Apr 20 20:42:41 2023|main.py|INFO] EPOCH [25/100] Learning rate: 0.00236
[Thu Apr 20 20:58:27 2023|main.py|INFO] EPOCH [26/100] Train NME: 0.13151
[Thu Apr 20 20:58:28 2023|main.py|INFO] EPOCH [26/100] Test NME: 0.15131
[Thu Apr 20 20:58:28 2023|main.py|INFO] EPOCH [26/100] Learning rate: 0.00189
[Thu Apr 20 21:23:26 2023|main.py|INFO] EPOCH [27/100] Train NME: 0.13091
[Thu Apr 20 21:23:30 2023|main.py|INFO] EPOCH [27/100] Test NME: 0.15042
[Thu Apr 20 21:23:30 2023|main.py|INFO] EPOCH [27/100] Learning rate: 0.00151
[Thu Apr 20 21:52:26 2023|main.py|INFO] EPOCH [28/100] Train NME: 0.13093
[Thu Apr 20 21:52:30 2023|main.py|INFO] EPOCH [28/100] Test NME: 0.15314
[Thu Apr 20 21:52:30 2023|main.py|INFO] EPOCH [28/100] Learning rate: 0.00121
[Thu Apr 20 22:21:26 2023|main.py|INFO] EPOCH [29/100] Train NME: 0.13116
[Thu Apr 20 22:21:30 2023|main.py|INFO] EPOCH [29/100] Test NME: 0.15046
[Thu Apr 20 22:21:30 2023|main.py|INFO] EPOCH [29/100] Learning rate: 0.00097
[Thu Apr 20 22:51:27 2023|main.py|INFO] EPOCH [30/100] Train NME: 0.13071
[Thu Apr 20 22:51:30 2023|main.py|INFO] EPOCH [30/100] Test NME: 0.15205
[Thu Apr 20 22:51:30 2023|main.py|INFO] EPOCH [30/100] Learning rate: 0.00077
