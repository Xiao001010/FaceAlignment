[Fri Apr 21 16:22:46 2023|main.py|INFO] Task: Cas_Stage1_Aug_Wing_lr0.5_B4
[Fri Apr 21 16:22:46 2023|main.py|INFO] Training cascade stage 1
[Fri Apr 21 16:22:46 2023|main.py|INFO] Using device: cuda
[Fri Apr 21 16:22:46 2023|main.py|INFO] Using config: config\Cas_Stage1_Aug_Wing\Cas_Stage1_Aug_Wing_lr0.5_B4.yaml
[Fri Apr 21 16:22:46 2023|main.py|INFO] Train path: data/training_images_full_train.npz
[Fri Apr 21 16:22:46 2023|main.py|INFO] Train path 2: data/training_images_subset.npz
[Fri Apr 21 16:22:46 2023|main.py|INFO] Test path: data/training_images_full_test.npz
[Fri Apr 21 16:22:46 2023|main.py|INFO] Train augment: True
[Fri Apr 21 16:22:46 2023|main.py|INFO] Learning rate: 0.5
[Fri Apr 21 16:22:46 2023|main.py|INFO] Batch size: 4
[Fri Apr 21 16:22:46 2023|main.py|INFO] Num epochs: 100
[Fri Apr 21 16:22:46 2023|main.py|INFO] Save model: True
[Fri Apr 21 16:22:46 2023|main.py|INFO] Loss: Wing
[Fri Apr 21 16:22:46 2023|main.py|INFO] Log path: logs/Cas_Stage1_Aug_Wing_lr0.5_B4/2023-04-21_16-22-46.log
[Fri Apr 21 16:22:46 2023|main.py|INFO] Writer path: runs/Cas_Stage1_Aug_Wing_lr0.5_B4/2023-04-21_16-22-46
[Fri Apr 21 16:22:46 2023|main.py|INFO] Model name: resnet18
[Fri Apr 21 16:22:46 2023|main.py|INFO] Num outputs: 10
[Fri Apr 21 16:22:46 2023|main.py|INFO] Pretrained: True
[Fri Apr 21 16:22:46 2023|main.py|INFO] Load model: False
[Fri Apr 21 16:22:46 2023|main.py|INFO] Load path: None
[Fri Apr 21 16:22:46 2023|main.py|INFO] Loading data...
[Fri Apr 21 16:22:46 2023|main.py|INFO] Load dataset for cascade stage 1
[Fri Apr 21 16:22:50 2023|main.py|INFO] Initializing network resnet18 with 10 outputs...
[Fri Apr 21 16:22:50 2023|main.py|INFO] Network: ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[Fri Apr 21 16:22:50 2023|main.py|INFO] Initializing loss and optimizer...
[Fri Apr 21 16:22:50 2023|main.py|INFO] Loss: Wing
[Fri Apr 21 16:22:50 2023|main.py|INFO] Optimizer: Adam
[Fri Apr 21 16:22:50 2023|main.py|INFO] Initializing tensorboard writer at: runs/Cas_Stage1_Aug_Wing_lr0.5_B4/2023-04-21_16-22-46
[Fri Apr 21 16:22:50 2023|main.py|INFO] Training network...
[Fri Apr 21 16:38:58 2023|main.py|INFO] EPOCH [1/100] Train NME: 0.27559
[Fri Apr 21 16:38:59 2023|main.py|INFO] EPOCH [1/100] Test NME: 0.29142
[Fri Apr 21 16:38:59 2023|main.py|INFO] EPOCH [1/100] Learning rate: 0.50000
[Fri Apr 21 16:38:59 2023|main.py|INFO] EPOCH [1/100] NME improved from 1000.00000 to 0.29142
[Fri Apr 21 16:38:59 2023|main.py|INFO] EPOCH [1/100] Saving model to: checkpoints/Cas_Stage1_Aug_Wing_lr0.5_B4/2023-04-21_16-38-59_epoch_1_NME_0.29142.pth.tar
[Fri Apr 21 16:53:50 2023|main.py|INFO] EPOCH [2/100] Train NME: 0.21514
[Fri Apr 21 16:53:51 2023|main.py|INFO] EPOCH [2/100] Test NME: 0.24543
[Fri Apr 21 16:53:51 2023|main.py|INFO] EPOCH [2/100] Learning rate: 0.40000
[Fri Apr 21 16:53:51 2023|main.py|INFO] EPOCH [2/100] NME improved from 0.29142 to 0.24543
[Fri Apr 21 16:53:51 2023|main.py|INFO] EPOCH [2/100] Saving model to: checkpoints/Cas_Stage1_Aug_Wing_lr0.5_B4/2023-04-21_16-53-51_epoch_2_NME_0.24543.pth.tar
[Fri Apr 21 17:09:29 2023|main.py|INFO] EPOCH [3/100] Train NME: 0.18816
[Fri Apr 21 17:09:30 2023|main.py|INFO] EPOCH [3/100] Test NME: 0.16598
[Fri Apr 21 17:09:30 2023|main.py|INFO] EPOCH [3/100] Learning rate: 0.32000
[Fri Apr 21 17:09:30 2023|main.py|INFO] EPOCH [3/100] NME improved from 0.24543 to 0.16598
[Fri Apr 21 17:09:30 2023|main.py|INFO] EPOCH [3/100] Saving model to: checkpoints/Cas_Stage1_Aug_Wing_lr0.5_B4/2023-04-21_17-09-30_epoch_3_NME_0.16598.pth.tar
[Fri Apr 21 17:25:05 2023|main.py|INFO] EPOCH [4/100] Train NME: 0.17264
[Fri Apr 21 17:25:06 2023|main.py|INFO] EPOCH [4/100] Test NME: 0.18917
[Fri Apr 21 17:25:06 2023|main.py|INFO] EPOCH [4/100] Learning rate: 0.25600
[Fri Apr 21 17:39:45 2023|main.py|INFO] EPOCH [5/100] Train NME: 0.16424
[Fri Apr 21 17:39:46 2023|main.py|INFO] EPOCH [5/100] Test NME: 0.18936
[Fri Apr 21 17:39:46 2023|main.py|INFO] EPOCH [5/100] Learning rate: 0.20480
[Fri Apr 21 17:53:36 2023|main.py|INFO] EPOCH [6/100] Train NME: 0.16023
[Fri Apr 21 17:53:37 2023|main.py|INFO] EPOCH [6/100] Test NME: 0.16352
[Fri Apr 21 17:53:37 2023|main.py|INFO] EPOCH [6/100] Learning rate: 0.16384
[Fri Apr 21 17:53:37 2023|main.py|INFO] EPOCH [6/100] NME improved from 0.16598 to 0.16352
[Fri Apr 21 17:53:37 2023|main.py|INFO] EPOCH [6/100] Saving model to: checkpoints/Cas_Stage1_Aug_Wing_lr0.5_B4/2023-04-21_17-53-37_epoch_6_NME_0.16352.pth.tar
[Fri Apr 21 18:08:10 2023|main.py|INFO] EPOCH [7/100] Train NME: 0.15812
[Fri Apr 21 18:08:11 2023|main.py|INFO] EPOCH [7/100] Test NME: 0.16114
[Fri Apr 21 18:08:11 2023|main.py|INFO] EPOCH [7/100] Learning rate: 0.13107
[Fri Apr 21 18:08:11 2023|main.py|INFO] EPOCH [7/100] NME improved from 0.16352 to 0.16114
[Fri Apr 21 18:08:11 2023|main.py|INFO] EPOCH [7/100] Saving model to: checkpoints/Cas_Stage1_Aug_Wing_lr0.5_B4/2023-04-21_18-08-11_epoch_7_NME_0.16114.pth.tar
[Fri Apr 21 18:22:33 2023|main.py|INFO] EPOCH [8/100] Train NME: 0.15038
[Fri Apr 21 18:22:34 2023|main.py|INFO] EPOCH [8/100] Test NME: 0.14534
[Fri Apr 21 18:22:34 2023|main.py|INFO] EPOCH [8/100] Learning rate: 0.10486
[Fri Apr 21 18:22:34 2023|main.py|INFO] EPOCH [8/100] NME improved from 0.16114 to 0.14534
[Fri Apr 21 18:22:34 2023|main.py|INFO] EPOCH [8/100] Saving model to: checkpoints/Cas_Stage1_Aug_Wing_lr0.5_B4/2023-04-21_18-22-34_epoch_8_NME_0.14534.pth.tar
[Fri Apr 21 18:36:33 2023|main.py|INFO] EPOCH [9/100] Train NME: 0.14840
[Fri Apr 21 18:36:34 2023|main.py|INFO] EPOCH [9/100] Test NME: 0.15153
[Fri Apr 21 18:36:34 2023|main.py|INFO] EPOCH [9/100] Learning rate: 0.08389
[Fri Apr 21 18:49:57 2023|main.py|INFO] EPOCH [10/100] Train NME: 0.14664
[Fri Apr 21 18:49:58 2023|main.py|INFO] EPOCH [10/100] Test NME: 0.15287
[Fri Apr 21 18:49:58 2023|main.py|INFO] EPOCH [10/100] Learning rate: 0.06711
